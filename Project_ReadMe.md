# **운동 동작 분석 시스템**   

## **개요**  
이 프로젝트는 **Reference 운동 영상**과 **실시간 사용자의 운동 영상**을 비교하여 **운동 수행 정확도를 분석하는 시스템**입니다.  

### **동작 방식**  
1. **Reference 영상 처리**  
   - Reference 운동 영상을 서버에서 분석하여 **각 관절의 3D 좌표를 JSON 파일로 저장**  
   - 2D Pose Estimation 모델로 **MoveNet** 사용  
   - 3D Pose Lifting 모델로 **VideoPose3D** 사용  

2. **실시간 운동 분석**  
   - 사용자가 웹 앱에서 Reference 영상을 보며 운동 수행  
   - Reference 영상과 3D 좌표 JSON을 호출하여 실시간 비교  
   - 프레임마다 3D 관절 상대 좌표를 비교하여 점수를 계산  

3. **점수 채점 방식 (20프레임마다 평가, 10프레임 슬라이딩 윈도우 적용)**  
   - 점수는 20프레임마다 갱신하며, 최근 10프레임씩 슬라이딩 윈도우 방식으로 저장하여 누적 평가  
   - **Reference 영상과 실시간 사용자의 3D 좌표 차이를 계산**  
   - 계산된 거리 값을 DTW(Dynamic Time Warping) 알고리즘을 이용하여 정렬하여 비교  
   - 3D 좌표의 x, y, z 각각의 DTW 점수를 평균내어 **관절별 유사도 점수 산출**  
   - 최종적으로 모든 관절의 점수를 평균 내어 **100점 기준으로 환산**  

---


## **점수 계산 방식 (좌표 차이 기반 DTW 적용)**  
각 프레임에서 **Reference 3D 좌표**와 **실시간 3D 좌표** 간의 **x, y, z 차이**를 측정하여 점수를 계산합니다.  
맨해튼 거리를 사용하여  x, y, z **각각의 절댓값 차이**를 구한 후 DTW를 적용합니다.

### **좌표별 거리 계산 (코드 방식)**  
\[
d_x = |x_{\text{ref}} - x_{\text{user}}|, \quad
d_y = |y_{\text{ref}} - y_{\text{user}}|, \quad
d_z = |z_{\text{ref}} - z_{\text{user}}|
\]

여기서,  
- \( (x_{\text{ref}}, y_{\text{ref}}, z_{\text{ref}}) \) = Reference 영상의 특정 관절 좌표  
- \( (x_{\text{user}}, y_{\text{user}}, z_{\text{user}}) \) = 사용자의 실시간 관절 좌표  

---

### **100점 환산 공식**  
\[
\text{score} = 100 - \left(\frac{\text{DTW 점수} - 0.6}{2.5 - 0.6} \times 100\right)
\]
- 최소 기준값(0.6)보다 작은 경우 100점 부여  
- 최대 기준값(2.5)보다 크면 0점에 가까운 점수 부여  

---
 

---

## **기술 스택**  

| 기술 분야        | 사용 기술 |
|----------------|----------|
| **Pose Estimation (2D)**  | MoveNet |
| **3D Pose Lifting**  | VideoPose3D |
| **시간 비동기 처리**  | DTW (Dynamic Time Warping) |
| **운동 점수 채점**  | 3D 좌표 차이 기반 평가, 20프레임마다 갱신, 10프레임 슬라이딩 윈도우 |
| **백엔드 & 배포**  | Docker, FastAPI |
| **프론트엔드**  | React (웹 앱) |
| **데이터 저장**  | JSON |

---
